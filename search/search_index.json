{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Dafda is a small Kafka client library for .NET that provides some high-level conveniences to allow setting up producers and consumers in a consistent manner. Features \u00b6 Purpose built for Kafka alone. Built on top of Confluent.Kafka . Built with .NET Core's configuration, logging, & dependency injection in mind. Supports Kafka configuration from multiple sources (e.g. environment variables). Easy (\"plug-and-play\") registration of consumer and producer messages. Default (configurable) JSON message serialization/deserialization. Supports multiple typed producers akin to HttpClient factories. Consumer messages are consumed in the background using hosted services. Multiple consumers supported, which can be configured independently. Extendable Outbox pattern implementation.","title":"Introduction"},{"location":"#introduction","text":"Dafda is a small Kafka client library for .NET that provides some high-level conveniences to allow setting up producers and consumers in a consistent manner.","title":"Introduction"},{"location":"#features","text":"Purpose built for Kafka alone. Built on top of Confluent.Kafka . Built with .NET Core's configuration, logging, & dependency injection in mind. Supports Kafka configuration from multiple sources (e.g. environment variables). Easy (\"plug-and-play\") registration of consumer and producer messages. Default (configurable) JSON message serialization/deserialization. Supports multiple typed producers akin to HttpClient factories. Consumer messages are consumed in the background using hosted services. Multiple consumers supported, which can be configured independently. Extendable Outbox pattern implementation.","title":"Features"},{"location":"configuration/","text":"Configuration \u00b6 Defaults \u00b6 Dafda relies on the defaults of confluent-kafka-dotnet nuget package, which in turn relies on the defaults from librdkafka . Basic \u00b6 For consumers the most basic and required options are bootstrap.servers and group-id , which can be configured, e.g. in Startup.cs , and is usually enough for local development (see docker-compose.yml ) public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithBootstrapServers ( \"localhost:9092\" ); options . WithGroupId ( \"consumer-group-id\" ); // register message handlers ... }); } } For specific configuration options see Consumer , Producer , and Outbox respectively. Manual \u00b6 Manually added configuration options take precedence over other configuration sources Example of manually setting the auto.offset.reset to earliest in Startup.cs to read from the beginning of the topic: public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfiguration ( \"auto.offset.reset\" , \"earliest); // other configuration options... // register message handlers ... }); } } Environment \u00b6 Dafda is able to use environment variables to keep configuration minimal and reusable Configuring a new self-contained service by gathering configuration from environment variables: Name Value DEFAULT_KAFKA_BOOTSTRAP_SERVERS default.kafka.confluent.net:9092 SAMPLE_KAFKA_GROUP_ID sample-group-id Caviat Further configuration values is most likely required in a production setup. Simply add the following to Startup.cs : public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfigurationSource ( Configuration ); options . WithEnvironmentStyle ( \"DEFAULT_KAFKA\" , \"SAMPLE_KAFKA\" ); // register message handlers ... }); } } Under the hood Dafda will check all environment variables starting with DEFAULT_KAFKA_ or SAMPLE_KAFKA_ , and match the rest of the environment key against valid Kafka configurations (see Consumer Configuration , Producer Configuration or Outbox Configuration for more info) and apply them to the Consumer configuration used by Kafka. Conventions \u00b6 The default naming convention of environment variables is the concatenation of the optionally supplied prefix and the Kafka configuration key transformed into screaming snake case , where each word is in uppercase and separated by an underscore ( _ ). That means group.id could be supplied in the environment as PREFIX_GROUP_ID . Naming conventions are freely configurable Configuration keys can be transformed using the WithNamingConvention method: public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfigurationSource ( Configuration ); options . WithNamingConvention ( key => key . ToLower ()); // register message handlers ... }); } } Automatic Configuration Keys \u00b6 Dafda will automatically attempt to look for the following Kafka configuration: Key Consumer Producer bootstrap.servers x x group.id x n/a enable.auto.commit x n/a broker.version.fallback x x api.version.fallback.ms x x ssl.ca.location x x sasl.username x x sasl.password x x sasl.mechanisms x x security.protocol x x Custom \u00b6 Create a custom implementation of ConfigurationSource and implement the GetByKey method: public class CustomConfigurationSource : ConfigurationSource { public override string GetByKey ( string key ) { return \"some-configuration-value\" ; } } Use the custom implementation in Startup.cs : public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfigurationSource ( new CustomConfigurationSource ()); // further configurations... ... }); } }","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#defaults","text":"Dafda relies on the defaults of confluent-kafka-dotnet nuget package, which in turn relies on the defaults from librdkafka .","title":"Defaults"},{"location":"configuration/#basic","text":"For consumers the most basic and required options are bootstrap.servers and group-id , which can be configured, e.g. in Startup.cs , and is usually enough for local development (see docker-compose.yml ) public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithBootstrapServers ( \"localhost:9092\" ); options . WithGroupId ( \"consumer-group-id\" ); // register message handlers ... }); } } For specific configuration options see Consumer , Producer , and Outbox respectively.","title":"Basic"},{"location":"configuration/#manual","text":"Manually added configuration options take precedence over other configuration sources Example of manually setting the auto.offset.reset to earliest in Startup.cs to read from the beginning of the topic: public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfiguration ( \"auto.offset.reset\" , \"earliest); // other configuration options... // register message handlers ... }); } }","title":"Manual"},{"location":"configuration/#environment","text":"Dafda is able to use environment variables to keep configuration minimal and reusable Configuring a new self-contained service by gathering configuration from environment variables: Name Value DEFAULT_KAFKA_BOOTSTRAP_SERVERS default.kafka.confluent.net:9092 SAMPLE_KAFKA_GROUP_ID sample-group-id Caviat Further configuration values is most likely required in a production setup. Simply add the following to Startup.cs : public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfigurationSource ( Configuration ); options . WithEnvironmentStyle ( \"DEFAULT_KAFKA\" , \"SAMPLE_KAFKA\" ); // register message handlers ... }); } } Under the hood Dafda will check all environment variables starting with DEFAULT_KAFKA_ or SAMPLE_KAFKA_ , and match the rest of the environment key against valid Kafka configurations (see Consumer Configuration , Producer Configuration or Outbox Configuration for more info) and apply them to the Consumer configuration used by Kafka.","title":"Environment"},{"location":"configuration/#conventions","text":"The default naming convention of environment variables is the concatenation of the optionally supplied prefix and the Kafka configuration key transformed into screaming snake case , where each word is in uppercase and separated by an underscore ( _ ). That means group.id could be supplied in the environment as PREFIX_GROUP_ID . Naming conventions are freely configurable Configuration keys can be transformed using the WithNamingConvention method: public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfigurationSource ( Configuration ); options . WithNamingConvention ( key => key . ToLower ()); // register message handlers ... }); } }","title":"Conventions"},{"location":"configuration/#automatic-configuration-keys","text":"Dafda will automatically attempt to look for the following Kafka configuration: Key Consumer Producer bootstrap.servers x x group.id x n/a enable.auto.commit x n/a broker.version.fallback x x api.version.fallback.ms x x ssl.ca.location x x sasl.username x x sasl.password x x sasl.mechanisms x x security.protocol x x","title":"Automatic Configuration Keys"},{"location":"configuration/#custom","text":"Create a custom implementation of ConfigurationSource and implement the GetByKey method: public class CustomConfigurationSource : ConfigurationSource { public override string GetByKey ( string key ) { return \"some-configuration-value\" ; } } Use the custom implementation in Startup.cs : public class Startup { public Startup ( IConfiguration configuration ) { Configuration = configuration ; } public IConfiguration Configuration { get ; } public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithConfigurationSource ( new CustomConfigurationSource ()); // further configurations... ... }); } }","title":"Custom"},{"location":"consumer/","text":"Consumer \u00b6 Quick Start \u00b6 Setup consumer configuration Create a message class Create a message handler Setup consumer configuration \u00b6 Add Kafka consumer configuration and message handlers in Startup 's ConfigureServices() : public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithBootstrapServers ( \"http://localhost:9092\" ); options . WithGroupId ( \"consumer-group-id\" ); // register message handlers options . RegisterMessageHandler < Test , TestHandler >( \"test-topic\" , \"test-event\" ); }); } ... } Multiple consumers It is possible to add multiple consumers (with different configuration) using the AddConsumer() extension method, which might be helpful when consuming from different topic with a different auto.offset.reset . Create a message class \u00b6 Create a POCO representation of the Kafka message: public class Test { public string AggregateId { get ; set ; } } Private/public properties Due to the nature of System.Text.Json having private setters will result in properties being skipped by the default deserializer. Constructors Please use an public parameterless constructor to be on the safe side of System.Text.Json . Create a message handler \u00b6 Create a message handler that implements the IMessageHandler<Test> interface, and use the Handle() method to do all the needed business logic to handle the message: public class TestHandler : IMessageHandler < Test > { private readonly ILogger < TestHandler > _logger ; public TestHandler ( ILogger < TestHandler > logger ) { _logger = logger ; } public Task Handle ( Test message , MessageHandlerContext context ) { _logger . LogInformation ( @\"Handled: {@Message}\" , message ); return Task . CompletedTask ; } } MessageHandlerContext The MessageHandlerContext contains information about the message, such as: Message Identifier Correlation Identifier Message Type Message Headers Dependency Injection \u00b6 Types registered in the IServiceCollection are available as constructor arguments. The IMessageHandler<T> implementation and any dependencies are resolved as part of the same scope, and disposed at the end of the Handle() method invocation. DbContext Therefore, it is entirely possible to resolve DbContext instances and have them behave as expected, however, transaction management and calls to SaveChangesAsync() are left up to the user . Service Lifetimes As always, with dependency injection, be mindful of service lifetimes . Configuration \u00b6 Kafka Consumer Settings ( Confluent Docs ) Consumer Options \u00b6 Information about some of basic options are available in the general Configuration section. Message Handler Registration \u00b6 Adding message handlers during service configuration: public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { // register outgoing messages (includes outbox messages) options . RegisterMessageHandler < Test , TestHandler >( \"test-topic\" , \"test-event\" ); }); } Will ensure that all messages with the type 1 test-event on the Kafka topic named test-topic will be deserialized as an instance of the POCO Test and handed to a transiently resolved instance of TestHandler . This is all handled by the .NET Core dependency injection, and Dafda clients need only concern themselves with creating simple messages and matching message handlers. Unconfigured Messages \u00b6 By default, a consumer will throw a MissingMessageHandlerRegistrationException if it receives a message where the type has not been configured with a handler. This can be overridden by providing a different IUnconfiguredMessageHandlingStrategy : public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { // register outgoing messages (includes outbox messages) options . RegisterMessageHandler < Test , TestHandler >( \"test-topic\" , \"test-event\" ); // log, but perform no other action for other messages options . WithUnconfiguredMessageHandlingStrategy < UseNoOpHandler >(); }); } UseNoOpHandler is built in, and uses an ILogger to log an information message about having received the message and then considers it processed. Message Deserialization \u00b6 In order to gain controler over the deserialization of the message handled by Dafda use WithIncomingMessageFactory , like: public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { // register outgoing messages (includes outbox messages) options . WithIncomingMessageFactory ( new XmlMessageSerializer ()); }); } To override the default JSON deserializer, and supply a custom implementation of the IIncomingMessageFactory interface. Unit of Work Factory \u00b6 It is possible to override the default Unit of Work behavior for each consumed message, using configuration options and custom implementations of IHandlerUnitOfWorkFactory and IHandlerUnitOfWork . However, the default implementation adheres to the scoped service lifetime of .NET Core's dependency injection, allowing it to work in tandem with the scopes of, e.g., EF Core. For more information see ServiceProviderUnitOfWorkFactory.cs Unit of Work Factory \u00b6 When a consumer starts Dafda continues consuming from the last committed offset on each topic, if a committed offset exists. To read all topics from the beginning of all partitions, use ReadFromBeginning : public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { options . ReadFromBeginning (); }); } The message type is part of the Message Envelope \u21a9","title":"Consumer"},{"location":"consumer/#consumer","text":"","title":"Consumer"},{"location":"consumer/#quick-start","text":"Setup consumer configuration Create a message class Create a message handler","title":"Quick Start"},{"location":"consumer/#setup-consumer-configuration","text":"Add Kafka consumer configuration and message handlers in Startup 's ConfigureServices() : public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: consumer services . AddConsumer ( options => { // configuration settings options . WithBootstrapServers ( \"http://localhost:9092\" ); options . WithGroupId ( \"consumer-group-id\" ); // register message handlers options . RegisterMessageHandler < Test , TestHandler >( \"test-topic\" , \"test-event\" ); }); } ... } Multiple consumers It is possible to add multiple consumers (with different configuration) using the AddConsumer() extension method, which might be helpful when consuming from different topic with a different auto.offset.reset .","title":"Setup consumer configuration"},{"location":"consumer/#create-a-message-class","text":"Create a POCO representation of the Kafka message: public class Test { public string AggregateId { get ; set ; } } Private/public properties Due to the nature of System.Text.Json having private setters will result in properties being skipped by the default deserializer. Constructors Please use an public parameterless constructor to be on the safe side of System.Text.Json .","title":"Create a message class"},{"location":"consumer/#create-a-message-handler","text":"Create a message handler that implements the IMessageHandler<Test> interface, and use the Handle() method to do all the needed business logic to handle the message: public class TestHandler : IMessageHandler < Test > { private readonly ILogger < TestHandler > _logger ; public TestHandler ( ILogger < TestHandler > logger ) { _logger = logger ; } public Task Handle ( Test message , MessageHandlerContext context ) { _logger . LogInformation ( @\"Handled: {@Message}\" , message ); return Task . CompletedTask ; } } MessageHandlerContext The MessageHandlerContext contains information about the message, such as: Message Identifier Correlation Identifier Message Type Message Headers","title":"Create a message handler"},{"location":"consumer/#dependency-injection","text":"Types registered in the IServiceCollection are available as constructor arguments. The IMessageHandler<T> implementation and any dependencies are resolved as part of the same scope, and disposed at the end of the Handle() method invocation. DbContext Therefore, it is entirely possible to resolve DbContext instances and have them behave as expected, however, transaction management and calls to SaveChangesAsync() are left up to the user . Service Lifetimes As always, with dependency injection, be mindful of service lifetimes .","title":"Dependency Injection"},{"location":"consumer/#configuration","text":"Kafka Consumer Settings ( Confluent Docs )","title":"Configuration"},{"location":"consumer/#consumer-options","text":"Information about some of basic options are available in the general Configuration section.","title":"Consumer Options"},{"location":"consumer/#message-handler-registration","text":"Adding message handlers during service configuration: public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { // register outgoing messages (includes outbox messages) options . RegisterMessageHandler < Test , TestHandler >( \"test-topic\" , \"test-event\" ); }); } Will ensure that all messages with the type 1 test-event on the Kafka topic named test-topic will be deserialized as an instance of the POCO Test and handed to a transiently resolved instance of TestHandler . This is all handled by the .NET Core dependency injection, and Dafda clients need only concern themselves with creating simple messages and matching message handlers.","title":"Message Handler Registration"},{"location":"consumer/#unconfigured-messages","text":"By default, a consumer will throw a MissingMessageHandlerRegistrationException if it receives a message where the type has not been configured with a handler. This can be overridden by providing a different IUnconfiguredMessageHandlingStrategy : public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { // register outgoing messages (includes outbox messages) options . RegisterMessageHandler < Test , TestHandler >( \"test-topic\" , \"test-event\" ); // log, but perform no other action for other messages options . WithUnconfiguredMessageHandlingStrategy < UseNoOpHandler >(); }); } UseNoOpHandler is built in, and uses an ILogger to log an information message about having received the message and then considers it processed.","title":"Unconfigured Messages"},{"location":"consumer/#message-deserialization","text":"In order to gain controler over the deserialization of the message handled by Dafda use WithIncomingMessageFactory , like: public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { // register outgoing messages (includes outbox messages) options . WithIncomingMessageFactory ( new XmlMessageSerializer ()); }); } To override the default JSON deserializer, and supply a custom implementation of the IIncomingMessageFactory interface.","title":"Message Deserialization"},{"location":"consumer/#unit-of-work-factory","text":"It is possible to override the default Unit of Work behavior for each consumed message, using configuration options and custom implementations of IHandlerUnitOfWorkFactory and IHandlerUnitOfWork . However, the default implementation adheres to the scoped service lifetime of .NET Core's dependency injection, allowing it to work in tandem with the scopes of, e.g., EF Core. For more information see ServiceProviderUnitOfWorkFactory.cs","title":"Unit of Work Factory"},{"location":"consumer/#unit-of-work-factory_1","text":"When a consumer starts Dafda continues consuming from the last committed offset on each topic, if a committed offset exists. To read all topics from the beginning of all partitions, use ReadFromBeginning : public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddConsumer ( options => { options . ReadFromBeginning (); }); } The message type is part of the Message Envelope \u21a9","title":"Unit of Work Factory"},{"location":"installation/","text":"Installation \u00b6 The library is available on nuget.org and can be installed in several ways. Package Manager Console \u00b6 Go to package manager console and run the following: install-package dafda Or: Commandline \u00b6 In a terminal/console window navigate to the project folder and run the following dotnet cli command: dotnet add package dafda Or... Whatever you prefer.","title":"Installation"},{"location":"installation/#installation","text":"The library is available on nuget.org and can be installed in several ways.","title":"Installation"},{"location":"installation/#package-manager-console","text":"Go to package manager console and run the following: install-package dafda Or:","title":"Package Manager Console"},{"location":"installation/#commandline","text":"In a terminal/console window navigate to the project folder and run the following dotnet cli command: dotnet add package dafda Or... Whatever you prefer.","title":"Commandline"},{"location":"messages/","text":"Messages \u00b6 At their lowest, messages on Kafka are represented by bytes. However, the current version of Dafda only supports string representations . In order to send message on Kafka the following is \"required\": Data/info Comment Topic The name of the Kafka topic Partition Key A unique key that Kafka uses to guarantee ordering, amongst other Message The actual message to be transported Dafda adds a layer on top using a Message Envelope . Partition Keys \u00b6 Like the message body format, the partition keys are also represented as string . Message Envelope \u00b6 Messages are by default wrapped in a message envelope, which makes the consuming and producing of makes straight forward. Given the following C# snippet: var someMessage = new SomeMessage { SomeId = Guid . CreateNew (), SomeData = \"This is very important data\" } producer . Produce ( someMessage ); Dafda's default (out-of-the-box) implementation comes with a message envelope that is serialized/deserialized from JSON, like: { \"messageId\" : \"100bee5a-82af-4003-82a2-fa6e543de24f\" , \"type\" : \"some-message\" , \"data\" : { \"someId\" : \"538b7db6-54c0-4115-ab6d-583d9714a289\" , \"someData\" : \"This is very important data\" } } Message Id \u00b6 The message id is by default a GUID generated by Dafda, but can be overriden in the Producer Configuration . Message Headers \u00b6 Currently message headers are embedded in the message envelope, and not using the available Kafka message header, which is more in tune with the CloudEvents specification . Any message header added will, using the default serializer, be at the same level as the messageId , type and data properties as in the listing above. Thus the following C# snippet: var headers = new Dictionary < string , object > { [\"some-header\"] = \"some-value\" , [\"another-header\"] = \"another-value\" }; var someMessage = new SomeMessage { SomeId = Guid . CreateNew (), SomeData = \"This is very important data\" } producer . Produce ( someMessage , headers ); Will yield the following JSON: { \"messageId\" : \"100bee5a-82af-4003-82a2-fa6e543de24f\" , \"type\" : \"some-message\" , \"some-header\" : \"some-value\" , \"another-header\" : \"another-value\" , \"data\" : { \"someId\" : \"538b7db6-54c0-4115-ab6d-583d9714a289\" , \"someData\" : \"This is very important data\" } }","title":"Messages"},{"location":"messages/#messages","text":"At their lowest, messages on Kafka are represented by bytes. However, the current version of Dafda only supports string representations . In order to send message on Kafka the following is \"required\": Data/info Comment Topic The name of the Kafka topic Partition Key A unique key that Kafka uses to guarantee ordering, amongst other Message The actual message to be transported Dafda adds a layer on top using a Message Envelope .","title":"Messages"},{"location":"messages/#partition-keys","text":"Like the message body format, the partition keys are also represented as string .","title":"Partition Keys"},{"location":"messages/#message-envelope","text":"Messages are by default wrapped in a message envelope, which makes the consuming and producing of makes straight forward. Given the following C# snippet: var someMessage = new SomeMessage { SomeId = Guid . CreateNew (), SomeData = \"This is very important data\" } producer . Produce ( someMessage ); Dafda's default (out-of-the-box) implementation comes with a message envelope that is serialized/deserialized from JSON, like: { \"messageId\" : \"100bee5a-82af-4003-82a2-fa6e543de24f\" , \"type\" : \"some-message\" , \"data\" : { \"someId\" : \"538b7db6-54c0-4115-ab6d-583d9714a289\" , \"someData\" : \"This is very important data\" } }","title":"Message Envelope"},{"location":"messages/#message-id","text":"The message id is by default a GUID generated by Dafda, but can be overriden in the Producer Configuration .","title":"Message Id"},{"location":"messages/#message-headers","text":"Currently message headers are embedded in the message envelope, and not using the available Kafka message header, which is more in tune with the CloudEvents specification . Any message header added will, using the default serializer, be at the same level as the messageId , type and data properties as in the listing above. Thus the following C# snippet: var headers = new Dictionary < string , object > { [\"some-header\"] = \"some-value\" , [\"another-header\"] = \"another-value\" }; var someMessage = new SomeMessage { SomeId = Guid . CreateNew (), SomeData = \"This is very important data\" } producer . Produce ( someMessage , headers ); Will yield the following JSON: { \"messageId\" : \"100bee5a-82af-4003-82a2-fa6e543de24f\" , \"type\" : \"some-message\" , \"some-header\" : \"some-value\" , \"another-header\" : \"another-value\" , \"data\" : { \"someId\" : \"538b7db6-54c0-4115-ab6d-583d9714a289\" , \"someData\" : \"This is very important data\" } }","title":"Message Headers"},{"location":"outbox/","text":"Outbox \u00b6 This documentation will not dive into the pros/cons of the Outbox pattern, but merely offer Dafda's take on it. The Outbox Pattern For more information, see e.g. https://microservices.io/patterns/data/transactional-outbox.html Dafda Outbox Implementation \u00b6 Dafda offers a framework for implementing a custom Outbox, using whatever technology is required, as longs as a few key interfaces are implemented and configured. Whether dispatching of outbox message are handled in-process or out-of-band is entirely up to the Dafda client. Below is a diagram over the moving parts: The Service , e.g. an ASP.NET application, receives a command request that writes data to the domain model (the \"Aggregate\" ) in the relational database. Supplying a custom implementation of the IOutboxEntryRepository interface, outbox messages should be persisted as part of the same transaction as the domain model. The Outbox processor will (either by notifications or timeouts) process the \"unpublished\" outbox messages, and produce those on the \"Aggregate\" Kafka topic. Example \u00b6 For a comprehensive example using ASP.NET Core, EF Core, and Postgres to persist outbox message, including an out-of-band outbox processor, take a look at the OutOfBandOutboxMessaging for the required configuration, and outbox_ddl.sql for the Postgresql schema.","title":"Outbox"},{"location":"outbox/#outbox","text":"This documentation will not dive into the pros/cons of the Outbox pattern, but merely offer Dafda's take on it. The Outbox Pattern For more information, see e.g. https://microservices.io/patterns/data/transactional-outbox.html","title":"Outbox"},{"location":"outbox/#dafda-outbox-implementation","text":"Dafda offers a framework for implementing a custom Outbox, using whatever technology is required, as longs as a few key interfaces are implemented and configured. Whether dispatching of outbox message are handled in-process or out-of-band is entirely up to the Dafda client. Below is a diagram over the moving parts: The Service , e.g. an ASP.NET application, receives a command request that writes data to the domain model (the \"Aggregate\" ) in the relational database. Supplying a custom implementation of the IOutboxEntryRepository interface, outbox messages should be persisted as part of the same transaction as the domain model. The Outbox processor will (either by notifications or timeouts) process the \"unpublished\" outbox messages, and produce those on the \"Aggregate\" Kafka topic.","title":"Dafda Outbox Implementation"},{"location":"outbox/#example","text":"For a comprehensive example using ASP.NET Core, EF Core, and Postgres to persist outbox message, including an out-of-band outbox processor, take a look at the OutOfBandOutboxMessaging for the required configuration, and outbox_ddl.sql for the Postgresql schema.","title":"Example"},{"location":"producer/","text":"Producer \u00b6 Quick Start \u00b6 Setup producer configuration Create a message class Produce messages Setup producer configuration \u00b6 Add Kafka producer configuration and outgoing messages: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { // configuration settings options . WithBootstrapServers ( \"localhost:9092\" ); // register outgoing messages (includes outbox messages) options . Register < Test >( \"test-topic\" , \"test-event\" , @event => @event . AggregateId ); }); } } Producers are registered as a per service registration The Service is registered with the .NET dependency injection, and able to produce Test messages on the Kafka topic test-topic . Create a message class \u00b6 Create a POCO representation of the Kafka message: public class Test { public string AggregateId { get ; set ; } } Produce messages \u00b6 Inject a dependency on Producer in the Service class and call the Produce method: public class Service { private readonly Producer _producer ; public Service ( Producer producer ) { _producer = producer ; } public async Task DoStuff () { ... await _producer . Produce ( new Test { AggregateId = \"aggregate-id\" }); ... } } Configuration \u00b6 Kafka Producer Settings ( Confluent Docs ) Producer Options \u00b6 Information about some of basic options are available in the general Configuration section. Message Registration \u00b6 It may seem a bit perculiar that outgoing (produced) messages needs to be registered, however, the current version of Dafda opted for a centralized configuration scheme. In order to produce message, messages needs to be registered for a given producer, like: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { // configuration settings options . WithBootstrapServers ( \"localhost:9092\" ); // register outgoing messages (includes outbox messages) options . Register < Test >( \"test-topic\" , \"test-event\" , @event => @event . AggregateId ); }); } } The class Test is now register for the Service producer, and able to send test-event message on the Kafka topic test-topic . The lambda function at the end of the Register call is used to select the partition key, so that ordering is applicable. Message Id Generator \u00b6 Each message should contain a unique way of identifying a specific instances of a message. The message id is pass along in the Message Envelope , and consumers should be able to use it for, e.g., deduplication. The default MessageIdGenerator generates a GUID for each call, deriving from the MessageIdGenerator class and implementing the NextMessageId method allows for overriding the default behavior, like: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { options . WithMessageIdGenerator ( new CustomMessageIdGenerator ()); }); } } Message Serialization \u00b6 It some possible to override the default JSON message serialization (here is more on Message Deserialization ). To implement a custom message serializer implement the IPayloadSerializer interface: /// <summary> /// Implementations must use the message payload and metadata in the /// <see cref=\"PayloadDescriptor\"/> in order to create a string representation. /// The MIME type format should be described by the <see cref=\"PayloadFormat\"/> /// property. /// </summary> public interface IPayloadSerializer { /// <summary> /// The MIME type of the payload format /// </summary> string PayloadFormat { get ; } /// <summary> /// Serialize the payload using the message and metadata in the /// <see cref=\"PayloadDescriptor\"/> /// </summary> /// <param name=\"payloadDescriptor\">The payload description</param> /// <returns>A string representation of the payload</returns> Task < string > Serialize ( PayloadDescriptor payloadDescriptor ); } And use the following to replace the default message serialization for all messages: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { options . WithDefaultPayloadSerializer ( new XmlPayloadSerializer ()); }); } } Or the following to replace message serialization for only a specific topic: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { options . WithPayloadSerializer ( \"test-topic\" , new XmlPayloadSerializer ()); }); } }","title":"Producer"},{"location":"producer/#producer","text":"","title":"Producer"},{"location":"producer/#quick-start","text":"Setup producer configuration Create a message class Produce messages","title":"Quick Start"},{"location":"producer/#setup-producer-configuration","text":"Add Kafka producer configuration and outgoing messages: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { // configuration settings options . WithBootstrapServers ( \"localhost:9092\" ); // register outgoing messages (includes outbox messages) options . Register < Test >( \"test-topic\" , \"test-event\" , @event => @event . AggregateId ); }); } } Producers are registered as a per service registration The Service is registered with the .NET dependency injection, and able to produce Test messages on the Kafka topic test-topic .","title":"Setup producer configuration"},{"location":"producer/#create-a-message-class","text":"Create a POCO representation of the Kafka message: public class Test { public string AggregateId { get ; set ; } }","title":"Create a message class"},{"location":"producer/#produce-messages","text":"Inject a dependency on Producer in the Service class and call the Produce method: public class Service { private readonly Producer _producer ; public Service ( Producer producer ) { _producer = producer ; } public async Task DoStuff () { ... await _producer . Produce ( new Test { AggregateId = \"aggregate-id\" }); ... } }","title":"Produce messages"},{"location":"producer/#configuration","text":"Kafka Producer Settings ( Confluent Docs )","title":"Configuration"},{"location":"producer/#producer-options","text":"Information about some of basic options are available in the general Configuration section.","title":"Producer Options"},{"location":"producer/#message-registration","text":"It may seem a bit perculiar that outgoing (produced) messages needs to be registered, however, the current version of Dafda opted for a centralized configuration scheme. In order to produce message, messages needs to be registered for a given producer, like: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { // configuration settings options . WithBootstrapServers ( \"localhost:9092\" ); // register outgoing messages (includes outbox messages) options . Register < Test >( \"test-topic\" , \"test-event\" , @event => @event . AggregateId ); }); } } The class Test is now register for the Service producer, and able to send test-event message on the Kafka topic test-topic . The lambda function at the end of the Register call is used to select the partition key, so that ordering is applicable.","title":"Message Registration"},{"location":"producer/#message-id-generator","text":"Each message should contain a unique way of identifying a specific instances of a message. The message id is pass along in the Message Envelope , and consumers should be able to use it for, e.g., deduplication. The default MessageIdGenerator generates a GUID for each call, deriving from the MessageIdGenerator class and implementing the NextMessageId method allows for overriding the default behavior, like: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { options . WithMessageIdGenerator ( new CustomMessageIdGenerator ()); }); } }","title":"Message Id Generator"},{"location":"producer/#message-serialization","text":"It some possible to override the default JSON message serialization (here is more on Message Deserialization ). To implement a custom message serializer implement the IPayloadSerializer interface: /// <summary> /// Implementations must use the message payload and metadata in the /// <see cref=\"PayloadDescriptor\"/> in order to create a string representation. /// The MIME type format should be described by the <see cref=\"PayloadFormat\"/> /// property. /// </summary> public interface IPayloadSerializer { /// <summary> /// The MIME type of the payload format /// </summary> string PayloadFormat { get ; } /// <summary> /// Serialize the payload using the message and metadata in the /// <see cref=\"PayloadDescriptor\"/> /// </summary> /// <param name=\"payloadDescriptor\">The payload description</param> /// <returns>A string representation of the payload</returns> Task < string > Serialize ( PayloadDescriptor payloadDescriptor ); } And use the following to replace the default message serialization for all messages: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { options . WithDefaultPayloadSerializer ( new XmlPayloadSerializer ()); }); } } Or the following to replace message serialization for only a specific topic: public class Startup { ... // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices ( IServiceCollection services ) { // configure messaging: producer services . AddProducerFor < Service >( options => { options . WithPayloadSerializer ( \"test-topic\" , new XmlPayloadSerializer ()); }); } }","title":"Message Serialization"}]}